---
title: Testing the agent application
description: This tutorial explains how you can test the agent loop with unit-tests and what other considerations you should have in mind when testing.
---

import { Aside, Card, Steps } from '@astrojs/starlight/components';

In this section we'll build a set of unit-tests and an integration test for the
agent loop we created in the previous sections. Testing is an important part of
building reliable agents and not always as straightforward as testing traditional
applications.

## Lab files

- **Starter:** [labs/agent-loop/003-agent-interface](/labs/agent-loop/003-agent-interface)
- **Solution:** [labs/agent-loop/004-testing-agents](/labs/agent-loop/004-testing-agents)

## Testing strategies for agents

Before we start writing tests for the coding agent, we need to cover test strategies for
agents in general. Agents differ from traditional software applications in many ways.
Because they use AI, the results aren’t stable, and you can’t just run unit tests
without solving the issues that arise from the fact that the AI responds differently
each time.

For the most part, you can follow the testing pyramid when defining a testing strategy
for AI agents:

- **Unit-tests:** Components that use rules without any AI can still be unit-tested 
as you normally would. For components that involve an interaction with the LLM, I recommend
using a mock. We'll try this out during the lab.

- **Integration tests:** Integration tests should focus on contracts and not exact results.
  You can't expect the same output from an LLM each time. This may sound like a obvious statement,
  but I've found that I have to remind me of this often when writing tests for agents.

  There is some good news on this front though. Some LLM providers are starting to offer
  "deterministic" modes where the same input will yield the same output each time.
  In Semantic Kernel you can set a Seed property to enable deterministic responses. This
  will make things somewhat easier when writing tests.

- **Simulation tests:** The top of the test pyramid is often implemented with end-to-end
  tests. It's quite difficult to build simple end-to-end tests with agents as the results
  are likely to vary. 
  
  To solve this, you can build simulation tests that check for the end result rather
  than the steps taken to get there. You can use an LLM to ask questions about the end
  state of the simulation to verify that you got results that make some sense.

  For the coding agent, you can for example configure the agent to run with a specific
  set of code and instructions and then use a second agent to analyze the outcome of the
  coding agent.

  <Aside type="note">
  Something is still nagging me when I'm explaining simulation tests: Can you truly
  trust an agent to verify another agent? The truth is that you probably can't. But
  research shows that LLMs will get it right around 80% of the time. So it's still
  valuable. But you should be aware of the limitations.
  </Aside>

In this lab we'll limit ourselves to building unit-tests. But I want to challenge you to
think about how you would setup a simulation test for the coding agent as a thinking
exercise.

## Creating fake objects for testing

In this section, you'll create a test project and build helper utilities to simplify
testing the agent loop. You'll start by setting up the test project, then create a
factory class that generates test objects like fake kernels and mock chat responses.

### Setting up the test project

<Steps>

1. Create a new xUnit test project named `CodingAgent.Tests` in the solution directory.
   Use the `dotnet new xunit` command to scaffold the project.

2. Add a project reference from `CodingAgent.Tests` to the `CodingAgent` project so you
   can test the agent implementation.

3. Install the Moq package to help mock dependencies. Use `dotnet add package Moq` to
   add it to your test project.

4. Delete the default `UnitTest1.cs` file that was created by the template, as you'll
   be creating your own test classes.

</Steps>

### Building the TestObjectFactory

The `TestObjectFactory` will provide reusable methods for creating test objects. This
keeps your test code clean and reduces duplication.

<Steps>

1. Create a new class called `TestObjectFactory` in the test project. This will be a
   static utility class containing factory methods.

2. Implement a `CreateTestKernel` method that accepts an `IChatCompletionService`
   parameter and returns a configured `Kernel` instance. Use the [Kernel builder pattern](https://learn.microsoft.com/en-us/semantic-kernel/concepts/kernel)
   to create the kernel and register the chat completion service as a singleton.

3. Create a `CreateFakeToolCall` method that returns a `ChatMessageContent` object
   containing a function call. This method should accept:
   
   - Plugin name (`string`)
   - Tool name (`string`)
   - Optional content (`string`)
   - Optional arguments (`Dictionary<string, object?>`)

   Use `FunctionCallContent` to represent the tool call and add it to a
   `ChatMessageContentItemCollection`. If content is provided, also add a `TextContent`
   item. Return a new `ChatMessageContent` with `AuthorRole.Assistant` and the items
   collection.

4. Add a `CreateFakeAgentResponse` method that accepts a string content parameter and
   returns a simple `ChatMessageContent` object with `AuthorRole.Assistant` and the
   provided content. This simulates a text response from the agent without any tool calls.

</Steps>

<Aside type="tip">
The TestObjectFactory encapsulates the complexity of creating test doubles for Semantic
Kernel objects. By centralizing this logic, you make your tests more maintainable and
easier to read.
</Aside>

## Testing tool calling functionality

Now that you have helper utilities for creating test objects, you'll write your first
test to verify the agent correctly invokes tools when the LLM requests them.

### Creating the test class

<Steps>

1. Create a new class called `AgentTests` in the test project. This will contain all
   your agent unit tests.

2. Add a test method called `Test_PromptCausingToolCall_InvokesTool` decorated with the
   `[Fact]` attribute. This test will verify that when the LLM returns a function call,
   the agent properly invokes that function.

</Steps>

### Mocking the chat completion service

The key to testing the agent loop is controlling what the LLM returns. You'll use Moq
to create a mock chat completion service that returns predefined responses.

<Steps>

1. Create a mock `IAgentCallbacks` object using Moq. This will allow you to verify that
   the agent reports function calls correctly.

2. Create a mock `IChatCompletionService` using Moq. This will simulate the LLM's
   responses.

3. Use `TestObjectFactory.CreateTestKernel` to create a kernel with your mock chat
   completion service.

4. Instantiate the `Agent` class with the test kernel and a new `AgentInstructions`
   instance.

</Steps>

### Setting up fake tools

<Steps>

1. Add a test plugin to the kernel using `kernel.Plugins.AddFromFunctions`. Create a
   plugin named "TestPlugin" with a single function called "test_function" that does
   nothing. Use `KernelFunctionFactory.CreateFromMethod` to create the function from
   a lambda expression.

2. Configure the mock chat completion service using `SetupSequence` on the
   `GetChatMessageContentsAsync` method. The sequence should return:
   - First call: A fake tool call (using `TestObjectFactory.CreateFakeToolCall`) for
     "TestPlugin.test_function"
   - Second call: A fake agent response (using `TestObjectFactory.CreateFakeAgentResponse`)
     with "All done!"

   This simulates the agent loop: first the LLM requests a tool, then after the tool
   executes, the LLM provides a final response.

</Steps>

<Aside type="note">
The `SetupSequence` method is crucial here because the agent loop calls the chat
completion service multiple times. Each call in the sequence returns a different
response, simulating a realistic conversation flow.
</Aside>

### Writing the test assertion

<Steps>

1. Invoke the agent with a test prompt and the mock callbacks object.

2. Use `agentCallbacks.Verify` to assert that `ReportFunctionCallAsync` was called with
   the function name "test_function". Use `It.Is<string>()` to match the function name
   and `It.IsAny<>()` for other parameters you don't need to verify strictly.

</Steps>

Your first test is complete! Run it with `dotnet test` to verify it passes.

## Testing agent responses

Not all prompts result in tool calls. Sometimes the agent simply returns a text
response. In this section, you'll write tests to verify the agent handles different
response scenarios correctly.

### Testing responses without tool calls

<Steps>

1. Create a new test method called `Test_PromptWithoutToolCall_ReturnsResponse` in your
   `AgentTests` class.

2. Set up the same mocks as before (agent callbacks, chat completion service, kernel,
   and agent instance), but this time skip adding any plugins to the kernel.

3. Configure the mock chat completion service to return only a fake agent response
   (no tool calls). Use `Setup` instead of `SetupSequence` since there's only one
   response needed.

4. Invoke the agent with a test prompt.

5. Verify that `ReportAgentResponseAsync` was called with the expected content
   ("All done!").

6. Verify that `ReportFunctionCallAsync` was never called by using
   `Times.Never()` in your verification.

</Steps>

### Testing mixed responses

Some LLM responses contain both text content and function calls. This happens when the
agent explains what it's doing while also requesting tool execution.

<Steps>

1. Add a test method called `Test_PromptWithToolsAndContent_CallsToolAndReturnsResponse`.

2. Reuse the setup pattern from the first test, including adding the test plugin.

3. Configure the mock chat completion service sequence to return:
   - First call: A fake tool call that includes both a function call and text content
     (e.g., "Invoking the test function"). Use the optional content parameter in
     `CreateFakeToolCall`.
   - Second call: A fake agent response with "All done!"

4. Invoke the agent with a test prompt.

5. Verify that `ReportAgentResponseAsync` was called exactly twice (once for the content
   during the tool call, once for the final response). Use `Times.Exactly(2)`.

6. Verify that `ReportFunctionCallAsync` was called exactly once.

</Steps>

<Aside type="tip">
These three tests cover the main scenarios in the agent loop: tool calls only, text
responses only, and mixed responses. This gives you confidence that the core agent
behavior works correctly without needing to call a real LLM.
</Aside>

## Summary

In this lab, you built a comprehensive test suite for the agent loop using unit tests
with mocked dependencies. You learned how to:

- Create a test project with xUnit and Moq for testing Semantic Kernel applications
- Build reusable factory methods to generate test objects like fake kernels, tool calls,
  and agent responses
- Mock the `IChatCompletionService` to control LLM responses during testing
- Use `SetupSequence` to simulate multi-turn agent conversations
- Verify that the agent correctly handles tool calls, text responses, and mixed responses
- Test callback invocations to ensure the agent reports its actions properly

Testing agents presents unique challenges because of the non-deterministic nature of
LLMs. By using mocks for unit tests, you can verify the agent loop logic without
depending on external services. This approach gives you fast, reliable tests that run
consistently.

As you continue building more complex agents, remember the testing pyramid:
- Unit tests with mocks for agent loop logic
- Integration tests for contracts and workflows
- Simulation tests for end-to-end scenarios

For more information on testing patterns with Semantic Kernel, refer to the
[Unit Testing with Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel/unit-testing-with-semantic-kernel/)
article on the Microsoft DevBlogs.

**Solution code:** `/labs/agent-loop/004-testing-agents/`